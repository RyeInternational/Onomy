{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>full_link</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>over_18</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>num_crossposts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>paradisenoir</td>\n",
       "      <td>1469810090</td>\n",
       "      <td>self.Adulting</td>\n",
       "      <td>https://www.reddit.com/r/Adulting/comments/4v7...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>I seriously didn't know where to put this ques...</td>\n",
       "      <td>Adulting</td>\n",
       "      <td>Need help with Updating driver's License</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>emptysometimes</td>\n",
       "      <td>1469550546</td>\n",
       "      <td>youtu.be</td>\n",
       "      <td>https://www.reddit.com/r/Adulting/comments/4up...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adulting</td>\n",
       "      <td>Adulting 101</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PortlandPerson94</td>\n",
       "      <td>1468814980</td>\n",
       "      <td>self.Adulting</td>\n",
       "      <td>https://www.reddit.com/r/Adulting/comments/4td...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>How to do laundry, organize your filing cabine...</td>\n",
       "      <td>Adulting</td>\n",
       "      <td>in the spirit of the discription of this sub, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>thisisthetyty</td>\n",
       "      <td>1467816249</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>https://www.reddit.com/r/Adulting/comments/4ri...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adulting</td>\n",
       "      <td>New youtube video about symptoms of adulting. ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>writer318</td>\n",
       "      <td>1467729176</td>\n",
       "      <td>theodysseyonline.com</td>\n",
       "      <td>https://www.reddit.com/r/Adulting/comments/4rc...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adulting</td>\n",
       "      <td>Why Must We Have To \"Adult\"?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9397</th>\n",
       "      <td>9397</td>\n",
       "      <td>PortlandPerson94</td>\n",
       "      <td>1468814980</td>\n",
       "      <td>self.Adulting</td>\n",
       "      <td>https://www.reddit.com/r/Adulting/comments/4td...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>How to do laundry, organize your filing cabine...</td>\n",
       "      <td>Adulting</td>\n",
       "      <td>in the spirit of the discription of this sub, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9398</th>\n",
       "      <td>9398</td>\n",
       "      <td>thisisthetyty</td>\n",
       "      <td>1467816249</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>https://www.reddit.com/r/Adulting/comments/4ri...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adulting</td>\n",
       "      <td>New youtube video about symptoms of adulting. ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9399</th>\n",
       "      <td>9399</td>\n",
       "      <td>writer318</td>\n",
       "      <td>1467729176</td>\n",
       "      <td>theodysseyonline.com</td>\n",
       "      <td>https://www.reddit.com/r/Adulting/comments/4rc...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adulting</td>\n",
       "      <td>Why Must We Have To \"Adult\"?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9400</th>\n",
       "      <td>9400</td>\n",
       "      <td>apolloxijedi</td>\n",
       "      <td>1467437994</td>\n",
       "      <td>self.Adulting</td>\n",
       "      <td>https://www.reddit.com/r/Adulting/comments/4qw...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>So the short version of the story my fiance an...</td>\n",
       "      <td>Adulting</td>\n",
       "      <td>can't believe I'm 30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9401</th>\n",
       "      <td>9401</td>\n",
       "      <td>apolloxijedi</td>\n",
       "      <td>1467388840</td>\n",
       "      <td>self.Adulting</td>\n",
       "      <td>https://www.reddit.com/r/Adulting/comments/4qs...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>I am tired of myself and other adults not be o...</td>\n",
       "      <td>Adulting</td>\n",
       "      <td>Can't we just be open and honest</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9402 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0            author  created_utc                domain  \\\n",
       "0              0      paradisenoir   1469810090         self.Adulting   \n",
       "1              1    emptysometimes   1469550546              youtu.be   \n",
       "2              2  PortlandPerson94   1468814980         self.Adulting   \n",
       "3              3     thisisthetyty   1467816249           youtube.com   \n",
       "4              4         writer318   1467729176  theodysseyonline.com   \n",
       "...          ...               ...          ...                   ...   \n",
       "9397        9397  PortlandPerson94   1468814980         self.Adulting   \n",
       "9398        9398     thisisthetyty   1467816249           youtube.com   \n",
       "9399        9399         writer318   1467729176  theodysseyonline.com   \n",
       "9400        9400      apolloxijedi   1467437994         self.Adulting   \n",
       "9401        9401      apolloxijedi   1467388840         self.Adulting   \n",
       "\n",
       "                                              full_link  num_comments  \\\n",
       "0     https://www.reddit.com/r/Adulting/comments/4v7...             1   \n",
       "1     https://www.reddit.com/r/Adulting/comments/4up...             0   \n",
       "2     https://www.reddit.com/r/Adulting/comments/4td...             1   \n",
       "3     https://www.reddit.com/r/Adulting/comments/4ri...             0   \n",
       "4     https://www.reddit.com/r/Adulting/comments/4rc...             0   \n",
       "...                                                 ...           ...   \n",
       "9397  https://www.reddit.com/r/Adulting/comments/4td...             1   \n",
       "9398  https://www.reddit.com/r/Adulting/comments/4ri...             0   \n",
       "9399  https://www.reddit.com/r/Adulting/comments/4rc...             0   \n",
       "9400  https://www.reddit.com/r/Adulting/comments/4qw...             3   \n",
       "9401  https://www.reddit.com/r/Adulting/comments/4qs...             1   \n",
       "\n",
       "      over_18  score                                           selftext  \\\n",
       "0       False      1  I seriously didn't know where to put this ques...   \n",
       "1       False      1                                                NaN   \n",
       "2       False      4  How to do laundry, organize your filing cabine...   \n",
       "3       False      1                                                NaN   \n",
       "4       False      1                                                NaN   \n",
       "...       ...    ...                                                ...   \n",
       "9397    False      4  How to do laundry, organize your filing cabine...   \n",
       "9398    False      1                                                NaN   \n",
       "9399    False      1                                                NaN   \n",
       "9400    False      4  So the short version of the story my fiance an...   \n",
       "9401    False      5  I am tired of myself and other adults not be o...   \n",
       "\n",
       "     subreddit                                              title  \\\n",
       "0     Adulting           Need help with Updating driver's License   \n",
       "1     Adulting                                       Adulting 101   \n",
       "2     Adulting  in the spirit of the discription of this sub, ...   \n",
       "3     Adulting  New youtube video about symptoms of adulting. ...   \n",
       "4     Adulting                       Why Must We Have To \"Adult\"?   \n",
       "...        ...                                                ...   \n",
       "9397  Adulting  in the spirit of the discription of this sub, ...   \n",
       "9398  Adulting  New youtube video about symptoms of adulting. ...   \n",
       "9399  Adulting                       Why Must We Have To \"Adult\"?   \n",
       "9400  Adulting                               can't believe I'm 30   \n",
       "9401  Adulting                   Can't we just be open and honest   \n",
       "\n",
       "      num_crossposts  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  \n",
       "...              ...  \n",
       "9397             NaN  \n",
       "9398             NaN  \n",
       "9399             NaN  \n",
       "9400             NaN  \n",
       "9401             NaN  \n",
       "\n",
       "[9402 rows x 12 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(r\"C:\\Users\\ymf\\Desktop\\pic16bGit\\rye_data_project_spring2021\\Reddit_dataset\\Adulting2016-2021.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Need help with Updating driver's License</td>\n",
       "      <td>I seriously didn't know where to put this ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adulting 101</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in the spirit of the discription of this sub, ...</td>\n",
       "      <td>How to do laundry, organize your filing cabine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New youtube video about symptoms of adulting. ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why Must We Have To \"Adult\"?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9397</th>\n",
       "      <td>in the spirit of the discription of this sub, ...</td>\n",
       "      <td>How to do laundry, organize your filing cabine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9398</th>\n",
       "      <td>New youtube video about symptoms of adulting. ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9399</th>\n",
       "      <td>Why Must We Have To \"Adult\"?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9400</th>\n",
       "      <td>can't believe I'm 30</td>\n",
       "      <td>So the short version of the story my fiance an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9401</th>\n",
       "      <td>Can't we just be open and honest</td>\n",
       "      <td>I am tired of myself and other adults not be o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9402 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0              Need help with Updating driver's License   \n",
       "1                                          Adulting 101   \n",
       "2     in the spirit of the discription of this sub, ...   \n",
       "3     New youtube video about symptoms of adulting. ...   \n",
       "4                          Why Must We Have To \"Adult\"?   \n",
       "...                                                 ...   \n",
       "9397  in the spirit of the discription of this sub, ...   \n",
       "9398  New youtube video about symptoms of adulting. ...   \n",
       "9399                       Why Must We Have To \"Adult\"?   \n",
       "9400                               can't believe I'm 30   \n",
       "9401                   Can't we just be open and honest   \n",
       "\n",
       "                                               selftext  \n",
       "0     I seriously didn't know where to put this ques...  \n",
       "1                                                   NaN  \n",
       "2     How to do laundry, organize your filing cabine...  \n",
       "3                                                   NaN  \n",
       "4                                                   NaN  \n",
       "...                                                 ...  \n",
       "9397  How to do laundry, organize your filing cabine...  \n",
       "9398                                                NaN  \n",
       "9399                                                NaN  \n",
       "9400  So the short version of the story my fiance an...  \n",
       "9401  I am tired of myself and other adults not be o...  \n",
       "\n",
       "[9402 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df.dropna(subset=['title'])[['title','selftext']]\n",
    "df2 #leave it to anaylze later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = df2[df2['selftext'].str.contains(\"full time job\")|df['title'].str.contains(\"full time job\")]\n",
    "job.to_csv('job.csv',index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = df2[df2['selftext'].str.contains(\"credit card\")|df['title'].str.contains(\"credit card\")]\n",
    "credit.to_csv('credit.csv',index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = df2[df2['selftext'].str.contains(\"don't know\")|df['title'].str.contains(\"don't know\")]\n",
    "dt.to_csv('dt.csv',index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 duplicated values.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6518"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take interesting columns\n",
    "df1=df[['selftext','num_comments','score','title']]\n",
    "#drop those with text=NA\n",
    "df1=df1.dropna(subset=['selftext'])\n",
    "#drop meaningless text\n",
    "df1=df1[(df1.selftext!='[removed]') & (df1.selftext!='[deleted]')].reset_index()\n",
    "#drop duplicate\n",
    "duplicate = df1.duplicated()\n",
    "print('There are', duplicate.sum(), 'duplicated values.')\n",
    "df1=df1.drop_duplicates()\n",
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markov=df1['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-66-7d6e16794c5e>:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  full_time = df1[df1['selftext'].str.contains(\"full time job\")|df['title'].str.contains(\"full time job\")]\n"
     ]
    }
   ],
   "source": [
    "full_time = df1[df1['selftext'].str.contains(\"full time job\")|df['title'].str.contains(\"full time job\")]\n",
    "full_time.to_csv('full_time_job.csv',index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-67-c047d844ecb7>:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  dont_know = df1[df1['selftext'].str.contains(\"dont know\")|df['title'].str.contains(\"dont know\")]\n"
     ]
    }
   ],
   "source": [
    "dont_know = df1[df1['selftext'].str.contains(\"dont know\")|df['title'].str.contains(\"dont know\")]\n",
    "dont_know.to_csv('dont_know.csv',index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-5e259662fbe7>:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  credit = df1[df1['selftext'].str.contains(\"credit card\")|df['title'].str.contains(\"credit card\")]\n"
     ]
    }
   ],
   "source": [
    "credit = df1[df1['selftext'].str.contains(\"credit card\")|df['title'].str.contains(\"credit card\")]\n",
    "credit.to_csv('credit_card.csv',index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       i seriously didn't know where to put this ques...\n",
       "1       how to do laundry, organize your filing cabine...\n",
       "2       so the short version of the story my fiance an...\n",
       "3       i am tired of myself and other adults not be o...\n",
       "4       finally mastering public transportation. i fee...\n",
       "                              ...                        \n",
       "6513    like which info do i give them? my policy numb...\n",
       "6514    i seriously didn't know where to put this ques...\n",
       "6515    how to do laundry, organize your filing cabine...\n",
       "6516    so the short version of the story my fiance an...\n",
       "6517    i am tired of myself and other adults not be o...\n",
       "Name: selftext, Length: 6518, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['selftext']=df1['selftext'].str.lower()\n",
    "df1['selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    text = text.encode('ascii', 'ignore').decode()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i seriously didn't know where to put this question, i hope this is an okay place. \\n\\na little background: so i'm 22 with a valid driver's license from my home state that still has my parents address on it. i actually moved to the state/town i live in several years ago, but have been a bit of a wanderer and rarely had a permanent address so i just didn't want to deal with the hassle of updating my driver's license.\\n\\n now it's time to do it and i think i may have trouble proving that i'm a resident of this state, even though i've been here for a few years. currently am living with a friend in the most permanent situation i've been in and probably will be for a while longer. he is the one on the lease and has bills in his name, and we only have like a verbal agreement to pay rent/bills, no contract. i don't have any bills of my own or official mail. my w-2 this year had an old address on it that i'm not associated at all with anymore. i have a pay stub, and could get my friend to write up something that says i pay rent at his place, but before i go to the dmv and waste my time i guess i wanna know if that's enough for them to consider me a resident? has anyone had any experience with this? the website gives a list of acceptable documents to prove residency but i don't have any of them/they don't apply to me. thanks!\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['selftext']=df1['selftext'].apply(lambda x: remove_emoji(x))\n",
    "df1['selftext'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove url, punctuation, and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using regular expression\n",
    "import re\n",
    "def remove_other(x):\n",
    "    x = re.sub(\"\\$\",\" \", x) #remove $\n",
    "    x = re.sub(\"https*\\S+\", \" \", x) #remove url\n",
    "    x = re.sub(\"\\'\\w+\", '', x) #remove i'm,we're,let's after the '\n",
    "    x = re.sub(\"[0-9]+\", '', x) #remove numbers\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i seriously didn know where to put this question, i hope this is an okay place. \\n\\na little background: so i  with a valid driver license from my home state that still has my parents address on it. i actually moved to the state/town i live in several years ago, but have been a bit of a wanderer and rarely had a permanent address so i just didn want to deal with the hassle of updating my driver license.\\n\\n now it time to do it and i think i may have trouble proving that i a resident of this state, even though i been here for a few years. currently am living with a friend in the most permanent situation i been in and probably will be for a while longer. he is the one on the lease and has bills in his name, and we only have like a verbal agreement to pay rent/bills, no contract. i don have any bills of my own or official mail. my w- this year had an old address on it that i not associated at all with anymore. i have a pay stub, and could get my friend to write up something that says i pay rent at his place, but before i go to the dmv and waste my time i guess i wanna know if that enough for them to consider me a resident? has anyone had any experience with this? the website gives a list of acceptable documents to prove residency but i don have any of them/they don apply to me. thanks!'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['selftext']=df1['selftext'].apply(lambda x: remove_other(x))\n",
    "df1['selftext'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'was', 'own', \"hadn't\", 'their', 'i', 'himself', 'mustn', 'were', \"needn't\", 'each', 'herself', 'its', \"you'd\", \"doesn't\", 'yours', 'below', 'how', \"she's\", 'so', 'did', 'very', 'no', 'the', 'your', 'into', 'for', 'doing', 'any', 'won', 'between', 'his', 'do', 'wouldn', 'not', 'while', \"didn't\", \"you've\", \"wasn't\", \"couldn't\", 'over', 'as', 'whom', 'o', 'than', 'don', 'does', 'to', 'here', 'shouldn', 'again', \"you're\", 'me', 's', 'at', 'through', 'most', 'll', \"it's\", 've', 'ain', 'what', \"shouldn't\", \"that'll\", 'they', 't', 'mightn', 'aren', 'ma', 'wasn', 'few', 'hadn', 'more', 'where', 'with', 'that', 'all', 'you', 'then', 'nor', 'once', 'because', 'until', 'been', 'from', 'too', 'isn', 'yourself', 'on', 'she', 'an', \"should've\", \"isn't\", \"shan't\", 'her', \"aren't\", 'itself', \"don't\", 'ourselves', 'has', 'theirs', 'above', 'a', 'now', 'these', 'out', \"hasn't\", 'have', 'under', 'such', \"you'll\", 'those', 'or', \"wouldn't\", \"won't\", 'about', 'can', 'is', 'why', 'only', 'we', 'doesn', 'needn', 'of', 'couldn', 'd', 'weren', 'them', \"haven't\", 'shan', 'having', 'both', 'up', 'm', 'before', 'my', 'and', 'our', 'just', 'be', 'but', 'it', 'by', 'there', 'should', 'which', 'y', 'who', 'had', 'yourselves', 'during', \"mightn't\", \"weren't\", 'ours', 'myself', 'he', 'further', 'when', 'this', 'if', \"mustn't\", 'are', 'will', 'themselves', 'him', 'am', 'against', 'other', 'hers', 'after', 'being', 'same', 'hasn', 'didn', 'down', 'off', 'in', 'some', 'haven', 're'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\") #uncomment it when run it for the first time\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words) #all preloaded stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seriously know put question, hope okay place. little background: valid driver license home state still parents address it. actually moved state/town live several years ago, bit wanderer rarely permanent address want deal hassle updating driver license. time think may trouble proving resident state, even though years. currently living friend permanent situation probably longer. one lease bills name, like verbal agreement pay rent/bills, contract. bills official mail. w- year old address associated anymore. pay stub, could get friend write something says pay rent place, go dmv waste time guess wanna know enough consider resident? anyone experience this? website gives list acceptable documents prove residency them/they apply me. thanks!'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['selftext']=df1['selftext'].apply(lambda x: remove_stopwords(x))\n",
    "df1['selftext'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       seriously know put question hope okay place li...\n",
       "1       laundry organize filing cabinet maintain finan...\n",
       "2       short version story fiance living parents put ...\n",
       "3           tired adults open honest other i e really you\n",
       "4       finally mastering public transportation feel g...\n",
       "                              ...                        \n",
       "6513    like info give them policy number agent phone ...\n",
       "6514    seriously know put question hope okay place li...\n",
       "6515    laundry organize filing cabinet maintain finan...\n",
       "6516    short version story fiance living parents put ...\n",
       "6517        tired adults open honest other i e really you\n",
       "Name: selftext, Length: 6518, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_chars = [\"!\",'â€œ','\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
    "              \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
    "              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n",
    "              \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\"]\n",
    "for char in spec_chars:\n",
    "    df1['selftext'] = df1['selftext'].str.replace(char, ' ')\n",
    "    df1['selftext'] = df1['selftext'].str.split().str.join(\" \")\n",
    "df1['selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the resulting text to see whatelse we need to clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finally mastering public transportation feel grown anyone else use moovit app rocks'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['selftext'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Lemmatization(change rules to rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finally ---> finally\n",
      "mastering ---> mastering\n",
      "public ---> public\n",
      "transportation ---> transportation\n",
      "feel ---> feel\n",
      "grown ---> grown\n",
      "anyone ---> anyone\n",
      "else ---> else\n",
      "use ---> use\n",
      "moovit ---> moovit\n",
      "app ---> app\n",
      "rocks ---> rock\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet') #uncommented for 1st time running\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words=df1['selftext'][4].split()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in words:\n",
    "    print(word + \" ---> \" + lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    nwords=[]\n",
    "    for word in words:\n",
    "        word=lemmatizer.lemmatize(word)\n",
    "        nwords.append(word)\n",
    "    return ' '.join(nwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finally mastering public transportation feel grown anyone else use moovit app rock'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['selftext']= df1['selftext'].str.split().apply(lambda x: lemmatize(x))\n",
    "df1['selftext'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaand</th>\n",
       "      <th>aaargh</th>\n",
       "      <th>aagh</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>aba</th>\n",
       "      <th>aback</th>\n",
       "      <th>abandon</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoology</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoopla</th>\n",
       "      <th>zootopia</th>\n",
       "      <th>zumiez</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zwillow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 16795 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaa  aaand  aaargh  aagh  aaron  ab  aba  aback  abandon  ...  zone  \\\n",
       "0   0    0      0       0     0      0   0    0      0        0  ...     0   \n",
       "1   0    0      0       0     0      0   0    0      0        0  ...     0   \n",
       "2   0    0      0       0     0      0   0    0      0        0  ...     0   \n",
       "3   0    0      0       0     0      0   0    0      0        0  ...     0   \n",
       "4   0    0      0       0     0      0   0    0      0        0  ...     0   \n",
       "\n",
       "   zoned  zoning  zoology  zoom  zoopla  zootopia  zumiez  zurich  zwillow  \n",
       "0      0       0        0     0       0         0       0       0        0  \n",
       "1      0       0        0     0       0         0       0       0        0  \n",
       "2      0       0        0     0       0         0       0       0        0  \n",
       "3      0       0        0     0       0         0       0       0        0  \n",
       "4      0       0        0     0       0         0       0       0        0  \n",
       "\n",
       "[5 rows x 16795 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec=CountVectorizer()\n",
    "counts=vec.fit_transform(df1['selftext'])\n",
    "counts=counts.toarray()\n",
    "count_df=pd.DataFrame(counts,columns=vec.get_feature_names())\n",
    "big_df=pd.concat((df1,count_df),axis=1)\n",
    "big_df = big_df.drop(['selftext','index','num_comments','score',\n",
    "                     'title'],axis = 1)\n",
    "big_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 835. MiB for an array with shape (6518, 16795) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-d2c9b42ab486>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"random\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbig_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#model1.components_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1312\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m         \"\"\"\n\u001b[1;32m-> 1314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, W, H)\u001b[0m\n\u001b[0;32m   1278\u001b[0m             \u001b[0mTransformed\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m         \"\"\"\n\u001b[1;32m-> 1280\u001b[1;33m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001b[0m\u001b[0;32m   1281\u001b[0m                                 dtype=[np.float64, np.float32])\n\u001b[0;32m   1282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m                     \u001b[1;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                 )\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1780\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1781\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 835. MiB for an array with shape (6518, 16795) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "model1=NMF(n_components=4,init=\"random\",random_state=0)\n",
    "model1.fit(big_df)\n",
    "#model1.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 835. MiB for an array with shape (6518, 16795) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-1882faf56cf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"random\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbig_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#model1.components_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1312\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m         \"\"\"\n\u001b[1;32m-> 1314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, W, H)\u001b[0m\n\u001b[0;32m   1289\u001b[0m             shuffle=self.shuffle)\n\u001b[0;32m   1290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1291\u001b[1;33m         self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,\n\u001b[0m\u001b[0;32m   1292\u001b[0m                                                     square_root=True)\n\u001b[0;32m   1293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py\u001b[0m in \u001b[0;36m_beta_divergence\u001b[1;34m(X, W, H, beta, square_root)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnorm_X\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnorm_WH\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2.\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcross_prod\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msquared_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msquare_root\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 835. MiB for an array with shape (6518, 16795) and data type float64"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "def top_words(X, model, component, num_words):\n",
    "    \"\"\"\n",
    "    Extract the top words from the specified component \n",
    "    for a topic model trained on data. \n",
    "    X: a term-document matrix, assumed to be a pd.DataFrame\n",
    "    model: a sklearn model with a components_ attribute, e.g. NMF\n",
    "    component: the desired component, specified as an integer. \n",
    "        Must be less than than the total number of components in model\n",
    "    num_words: the number of words to return.\n",
    "    \"\"\"\n",
    "    orders = np.argsort(model.components_, axis = 1)\n",
    "    important_words = np.array(X.columns)[orders]\n",
    "    return important_words[component][-num_words:]\n",
    "\n",
    "topic1=pd.DataFrame({'Topic 0':top_words(big_df, model1, 0, 5),\n",
    "                   'Topic 1':top_words(big_df, model1, 1, 5),\n",
    "                   'Topic 2':top_words(big_df, model1, 2, 5),\n",
    "                   'Topic 3':top_words(big_df, model1, 3, 5)})\n",
    "topic1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
